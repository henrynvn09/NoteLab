SPEAKER 0
We're gonna get started. Announcements. Um, first, project number one is due tonight. Remember, one person from your team should submit and link all the group members. And then Jon Johannes also sent out some instructions on um assigning pages. The diffusion assignment, which is project 2, has been released and this Friday, Kai Fung will hold a discussion section on project 2. It'll be at Bor A 103B at 30 p.m. right? And then on the agenda for today we're going to be finishing our little primer on graphical models and then we're gonna dive into the uh the the set up for diffusion models and then do some derivations right. Any questions on any course admin logistics? OK, let me recap where we were from last time. So last lecture I gave you this high level picture of what diffusion does, which is our goal will be to generate. An image from a distribution like the images of dogs and the way we're going to do this is we're going to first corrupt an image by turning a dog into unit Gaussian noise and. After we do this forward diffusion process of turning the dog into noise, we're going to reverse that process starting from noise and then be able to basically subtract away the noise to generate a dog at the output right? so that's the high level idea of diffusion. And then last lecture we said we'll need a few tools to get there. We have many of the tools, but one more thing that we need is uh graphical models. So we're doing this brief aside on graphical models. This will be really important for understanding the diffusion derivation. Uh, last lecture, we talked about graphical models where What we have is we have nodes. These are the circles with variables, random variables like A and B, and then they're connected by links or edges. And then we defined how from a graph you could write down. The probability density function of all the random variables. So if I have 5 random variables X1, X2, X3, X4X5, the way I write the probability of all of them is to write the probability of each variable given its parents. So I start off with X1. I write PFX1. It has no parents. PFX2 it has no parents PFX 3, it has no parents. Then PFX4 and X4's parents are X2 and X1 and X3. So it's PFX4 given it parents which are X1 X2 X3. And then we'll have a PF X5 given its parents and its parents are just X1 and X3. All right, so this is the factorization. Of a probability distribution. From a graph. And we also discussed last picture then. What's interesting in these graphs is the absence of links, right? The absence of links, like an absence of a link from X2 to X5 and X4 to X5, allows me to write this probability not as PFX5 given X1 X2 X3X4, but now it's just PF X5 given X1 and X3. So the absence of links allows us to simplify these probability distributions, all right. But Great, uh, the question is what is the vector representation here and how does it link to the probability. So here I wrote PF X where X is a vector with 5 random variables. This will still output of probability which is a scalar, uh, but this P of X, you can think of it as just expanding it. And so if X contains these 5 random variables, I'm running the the joint probability of all 5 random variables together. Great question. Other questions? OK, so we're going to cover a few things with these graphical models, uh, and then after that, we're I'm going to apply it to diffusion. So there are gonna be two questions we care about with graphical models. The first is independence. Not spelled independence correctly in uh independence. Um, If I have two random variables, A and B. If they're independent, I use this symbol and that denotes that A is independent of B. In colloquial terms, when I ask a question, is A independent of B? The question that you should have in your mind is There's knowing A Give me any additional information about B that I didn't already know. If it does, then A and B are not independent, but if knowing A gives me no additional information about B, then they are independent. In mathematical terms, Independence is as follows. If I write the joint probability of A and B. This factorizes as P. Of A times P of B. This is equivalent to saying, right? If I were to write out the chain rule of PFAcom B that would equal PFA times PFB given A, right? And so this is equivalent to saying that PFB. Given a equals PF. Um, and the Chain Robert in the other direction also holds so this would be PFA given B equals PFA, right? So that is independence. Any questions there? Should be refresher for everyone. OK, and then we also have something called conditional independence. This is gonna come up a lot in diffusion. Additional independence. Is written as the following. Let's say I have 3 random variables A, B, and C. But then I observed the random variable C, so conditioned on knowing the value of C. R A and B independent. So if A and B are independent, conditioned on me, knowing the value of C, then A and B are said to be conditionally independent. The way that we mathematically write this is we take the same independent statement, but we put a condition on C for every single expression. So this gives us P. Of A B C. Equals PFA given C. Times PFB C. All right. And just like these statements here. We have the equivalent thing for conditional independence. I'm just gonna put a condition on C behind every single uh probability expression. And so this is the equivalent to saying. That's uh P of B given C and A. equals P B C. And then PFA. Given to C and B Equals PFA given C. Right, so conditional independence. Is The same expressions as independence except everything is now conditioned on C. All right. Any questions there? OK, so we're going to study two examples. These will be the two examples that we'll encounter in class of uh two simple graphs that that will show us properties that we can generalize. So the first example is this graph where we have C. And it's the parent of A and B. That's example 1 and then example 2. Will be uh this graph where A is the parent. Of C, which is the parent of B. Alright? And then with all of with these two examples we're going to seek to answer 3 questions. The first question is. What is The factorized. Distribution Of all the variables of PF A, B, C, that's going to be us applying. This equation where we just do probability of each variable given as parents. And then we'll want to ask question number 2. Is A independent of B. And question number 3 is A. Additionally independent of B given C. All right, so we're gonna answer these three questions for the two example graphs, and that's gonna help us a lot for the diffusion derivation. Questions here. OK, let's move on then to the first example. So we're going to take our first graph. It has C as the parents of A and B. And my first question is what is the factorized distribution of P? Of A, B, and C. Someone raise your hand and tell me. Yeah. Great. So it's probability of C times the probability of A given C times the probability of B given C. And we can read that right off the graph, so that's correct because it's the probability of each node given its parents. Questions here? OK, let's get to some more interesting questions then. So the second question is, Is A independent of B. Now, we can go ahead and just start to plug in math to try to solve this, but before we do that, I want us to have an intuitive answer. So For the intuitive answer, then we ask this question. If A It's independent of B. And knowing A does not give me any information about the random variable b. That I did not already know, OK? So intuitively are A and B independent. Take 30 seconds to think about it, feel free to talk to your neighbor. I'm gonna do a hand raising poll after that. So intuitively are A and B independent. All right. I hear a lot of good discussion. Let's do a hand raising poll. Who here says, yes, A and B are independent? Who here says no A and B are not independent. OK, more people raise their hand for no, which is the correct answer. Intuitively, the answer is A and B are not independent. Can someone who got this correct. Give me an example or an intuition for why A and B in this case are not independent. 7 Uh either is fine. Great. So, the answer that Kevin gave is If we're considering that A, C, and B are binary variables, for example, knowing A happened or not, knowing A equals 1 should give me information on whether C happened or not, and then knowing whether C happened or not should influence B. And this is the general correct intuition. So because of this graph structure. A and C are linked, so knowing. I'm gonna be a bit more general sort of binary. I'm just gonna consider that A, B, and C are continuous valued random variables. Knowing the value of A is gonna give me some information about the value of C. And knowing the value of C because C is coupled to B, C influences B, knowing the value of C is gonna tell me some information about the value of B. OK, let me just actually write down an example so that I'm not just saying these things vaguely. Let's say that there was an actual model. Let's say that A equals 5 times C plus noise. OK, this is something that is a valid equation for this link. C is causal to A, so A is value is going to be influenced by the value of C. And then let's say that for this C to B link we had another equation that B equals C+ noise. So, in this example, Let's say that we got to observe the value of A. So if I observe the value of A, let's say I have to observe that A equals 1, OK. Then we're asking the question, does knowing the value of A give me any information about B that I didn't already know? Well, from knowing A equals 1. I can have a guess that C is probably in the ballpark of what 0.2, right? And then if C is in the ballpark of 0.2, then B should also be in the ballpark of 0.2. I say the ballpark because of these noise terms, right? The noise terms means I'm not gonna get C exactly or B exactly, but I'll be in the ballpark. So I did get some more information that I expect B to be around the value of 0.2. Therefore, knowing A gave me information about B. But I didn't already know and so they are dependent. Questions there. OK, so, if they're dependent, then I should not be able to show the following. So, PFAB. If they were independent, this would factorize as PFA. Times PFB. But we should not be able to show that this is true. So let's go ahead and actually Do the derivation. So I'll do it in purple. PFAB, I want to simplify this probability given this graph, OK. If I want to do that, what should be? A step that I take. Uh, the one student says condition on C. What do you mean by condition on C in this case? I introduce a term that conditions on C. yeah, great. So we want to introduce the term that conditions on C. I'll take that even more general step. I need a Compute PFA B specifically for this graph. All right. What do I know about this graph? I know that this graph factorizes according to this equation, right? And so I need it somewhere in my math simplify that PF ABC equals this thing because that's what's specific to this graph. So how do I get a PF ABC into this expression? Uh Uh, the question is, uh, the answer is, can I do PFA BC divided by PFC. So in this case, PF ABC. It's a good guess. P ABC divided by PFC is equal to PFAB given C, but here I have just PFA. B, so I can't use my chain rule here. How else can I introduce PF ABC into this expression? Perfect. Yeah. Sorry, can you remind me your name again? Edward. OK, so Edward says we can do a summation over C. So really, actually, let me just keep this here. Remember when we manipulate probabilities, there are really two things that will probably be The answer is either the chain rule, which is what this is, or the law of total probability, which is what Edward just said. So we could do some over C. Of PFA, B, and C. And that's the correct approach here. So what I'm going to now do is use Information about this graph PF ABC equals PFC. Times of a given C. Time P of B given C. And I want to see if I can simplify this thing to equal P of A times P of B. So the first thing that I'm going to do is I'm just gonna try to get a P of A out of here. How can I get a PA out of this expression? Yeah, you should. Um, so, That I can I get, you get PFC out by multiplying it by, are you talking about the by multiplying PSC and PFA given C? OK. Consolidating some of these expressions here is on the correct route. If I told you to from this expression, extract out of PFA, how would you do that? Mhm. Perfect. Yeah, that's correct. So, Answer is I can. Take these two terms. Remember my goal is to get out of PFA. So if I multiply these two together, right? I get PF A and C. And then that multiplies P of B given C. And then from my chain rule, I know that I can decompose this as PFA times PFC given A, right? And so this equals. Some overse Of PFA Times PFC given A. And then I have a times P of B given C. OK. Now because PFA doesn't depend on C, I can bring it out of the summation, and this equals. You have a Times some over C. Of P of C given A times P of B given C. All right? And remember, if they were independent, I would be able to simplify this to PFA times PB. But what we should see is that in general, In general, This Does not equal. P B. Right? What are some cases in which this expression does actually equal PFB? Great, so one example where it does equal PFB is if PFC given A equals PFC, right? That's because then we have PFC times PFB given C, which is PFB and C, and then I sum away C so I only get PFB, right? But PFC given A does not equal PFC because that violates this graph. This graph tells us that um the value of A will give us information about C, all right? And so. That is uh one way that this could equal PFB, but that is contradictory to the assumption of this graph. So in general, because this thing does not equal PFB, then we have shown that these are not independent. Right? Questions there. OK, so that's answering question two is A independent of B. We're going to now do question three, which is um. A and B conditionally independent given C. So the way that we'll draw this for a graph is as follows. I'll have a graph. The graph that we drew before. And when I observe a variable when it's behind the conditioning bar, what I'm going to do is I'm just gonna highlight that graph. I'm gonna shade it in with a color and that tells me that the value of C is observed. So C is no longer a random variable. C has a single value that I observe, OK? So the question is, are, A and B conditionally independent given C, and I want us to answer this first intuitively, and the intuitive question is to ask. Does knowing a. Sorry, A and B are conditionally independent given C if knowing A. Does not give me any more information about B than what I already knew from knowing C, all right? Take 30 seconds to think about that. Feel free to talk to your neighbor and then we'll do a hand raising poll again. Alright, let's, let's do a hand raising. Who says yes, A and B are conditionally independent given C? And then who says no, they are not conditionally independent given sea. All right, most people raise their hands for they are conditionally independent given see. And that is the correct answer. So can someone who said yes to this give me the intuition for why? David Perfect. Yeah, so David gives the right answer here, the correct intuition, which is in our prior example, knowing A gave me information about B through C, right? That was our example here, but in the case where I know C perfectly, knowing C perfectly already gives me information about B. If after knowing C perfectly, I get to observe A. I don't get any more information about B because the only way I could have would be that knowing A tells me something about C, which then tells me something about B, but I already knew C perfectly. So I already knew. Whatever I could about B from C. Therefore, A didn't give me any additional information, so they're conditionally independent. Does anyone want me to repeat that? All right, let's show it with math then. So if we want to show conditional independence, we want to show that PFA B given C. Equals if a given C. Times P B given C. I want to see if this is true. What's my first step? Pain rule perfect. And actually a student said it on the last slide that's why I wrote it down. PFAcom B given C equals PF ABC over PFC. So this equals um P A B C. Over PFC And then I'm gonna use then My factorized distribution for the graph of ABC equals PFC. Times PFA given C. Times PB given C. This is all divided by PFC. And then you can see clearly these PFCs cancel and I am therefore left with. A given C times P B C. Therefore, I can say that A. And B are conditionally independent given C for this particular graph. All right. Questions there. Yes. Uh, the question is, uh, does this matter that these are directed graphs? So for this class we will only do directed graphs for undirected graphs, uh, we don't have, it's not clear who is the parent and who is the child, and so that requires a different formulation and so it is important that these are directed. Oh great, yeah, that's a good question. So uh the question is if we reverse the arrows, does it change the conditional independencies? So in the next example, we're going to flip the arrow between A and C, and we're going to see that the, the same two answers that we got here are the same or are the same, which is that they are. Not independent but they are conditionally independent. But then there, there's gonna be a graph that I don't do in this class. I'll leave this as an exercise for you, but you can have a graph where. This is called the collider graph. We're not going to do this, but I'll tell you the answer. You could have A and B both colliding on a variable C. And in this case A and B are independent, but A and B are not conditionally independent given C and so this has the opposite answer and so the arrow directions do matter, and I'll leave that as an exercise for you guys if you're curious to see the answer come to my office hours. So let's do the other one. This is gonna be the graph that is really important for diffusion and reinforcement learning. Um, the first graph that we just did, example one, that'll also be important for reinforcement learning, but for diffusion we're really going to care about this graph. So this is a graph that has A being the parent of C, which is then the parent of B. And the first question we want to ask is what is the factorized distribution? We can just do this by inspection. PF ABC is PFA. Times of parent, oh sorry, PFC parent, PFC given A P of B given C. All right. Second question Is Uh R A and B. Independent. Intuitively The answer is no. For the same reason that we had for the prior graph. If I know A, that gives me some information about C. Knowing C gives me some information about B. So intuitively the answer here should be no. And let's go ahead and do some um Probability here. So if I want to write PFAcom B, we're gonna do the exact same thing we did in the prior example where we're going to use law of total probability some over for C. If A B C. This will equal some over C. We're gonna have PFA times PFC given A. Times P of B given C. Right. I can factor out the PFA. Or if not, I can pull it out of this summation cause it doesn't depend on C, and I have a sum over C. Of PFC given A times P of B given C. All right. And just like in the prior example, at this point, this term in general does not equal PFB and therefore these two are not. Independent. So just like it's actually the exact same expression, um. This expression in general. This Does Not Equal P of B. And so we can say that A and B are not independent. Questions there? OK, and then let's do the other one. Um, if I have the case that I have my graph A C to B, and now I observe the value of C. We asked the question, are A and B conditionally independent given C. And the intuition for this one. It's also the same as before. A gives me information about C which gives me information about B. But if I observe C. But I know its value. And if I know it's value, that gives me already a lot of information about the. Knowing A will not give me any more information about B because knowing A can only give me information about B through C, and I know C perfectly already. I got to observe it. All right. So, we'll write out the Formal math for this one. If a comb given C. Equals E of ABC divided by PFC. This is the first step from For example one just like before. And then I'm going to plug in my factorized distribution for PFABC. So this equals E A P C A P of B given C. And then this is divided by. E of C. All right. What should I do at this point to simplify this expression? Remember that my goal is to see does this thing equal question mark PF A given C times PB given C. If it does, then they are conditionally independent, and they should be because our intuition tells us they are conditionally independent. How do I simplify this expression to look like this? Yes. Perfect, yeah. So we're going to use our chain rule for the numerator. I can uh combine, I'll write this down in two steps just to be clear. uh PFA times PFC given A equals PFA com C. And PFB given C. And then to cancel out this PFC in the denominator, I can write my chain rule the other way. So this will be PFC times PFA given C times PFB given C. Divided by PFC. These PFCs now canceled out. And I am left with. BF A given C times P of B given C. And therefore, we do have for this graph that A is conditionally independent of the. and see. Right questions here. OK, so I really want you to remember this graph because what's going to happen in diffusion is we're going to see several graphs that look like the following. We're gonna have some variable X this could represent an image at some time sub zero and. I'm going to add noise to that to make it X1. I can add more noise to make it X2. I can add more noise to make it X3 and then eventually we turn it into gassian noise. I'm gonna call that X big T. This is going to be common in our diffusion models and what's gonna happen is I'm gonna want to write some probability distributions. Conditioned on others. So maybe I want to write the distribution of X3 conditioned on knowing X2. In this case, Right? Um If I were to write this joint distribution, it would be PF X0 times PX1 given X0 PX if I didn't know if I didn't know this graph, if I was just using the chain rule, I would have the PF X2 given X1 and X0 PX3 given X2 X1 X0. So using just a chain rule, I would have PX3 given X2 X1 X0. But then the key thing is because This graph tells me that x3. Is conditionally independent of X1. And actually not just X1 but anything to the left of X1, given that I know X2, this probability here simplifies to P of X3. Given X2. And this simplification is super important for All the derivations we're going to do. So basically in a diffusion model, if we observe a variable like X2. Then X3 will be conditionally independent of anything that comes before X2. Similarly, if I observe X3, then X4 is going to be conditionally of any conditionally independent of anything that comes before X3, all right? And this is something that we call. The Markov property. What the Markov property tells me is if I If I call time to the present time. Then knowing. By a random variable at the present time. Is all I need to know to get the distribution of the random variable the next time. I don't need to know any history before X2. All I need to know is X2. Once I know X2, X1, and X0, I can throw them away. Kevin. Yeah. Great. Uh, so, Kevin's question is, Uh, isn't PFX3 given X2 X1 X0 equal PFX3 given X2, a consequence of the graph we constructed, i.e., like if we just write the factorized distribution, we would have a PFX3 given X2 come out, right? And uh is there still a purpose of needing to know this conditional independence property that X3 will be conditionally independent of anything to the left of X2 given that I know X2? It will become useful because later on we will encounter distributions like the following. We're going to encounter uh a pea. For example, of X3, given X2 X0. And in this case, It's knowing the conditional independencies that allow me to erase this X0. Because I will know that X3 is conditionally independent of anything before X2, given that I know X2. So this thing will simplify also. To give X3 Give an X. Yeah, the, uh, Osy's question is, is the point that if we change these two graphs together we can use these simplifications to make conclusions. The answer is yes. So in diffusion we're only gonna have this graph, but in reinforcement learning, we're gonna also have another arrow that comes down, and that's gonna be our example one. So both of those will will factorize our probability distributions in a way that makes these problems tractable. David. Thank you, David. It's because I made a typo. That's a mistake. Yeah, this is PFX3 given X2. Thank you for that. Other questions? OK, remember, oh, Darren. Uh, the question is what do I mean by observe because if I had numbers in my model, wouldn't I know what the numbers are? So in this case we're just taking a very abstract view. C is a random variable, so maybe C takes on values like 1234 or 5, and it takes them on with some probability distribution. In this case, when we say C is observed, then we get to know the actual value that she takes on like C equals 2 or C equals 4 rather than C has this distribution of values 12345. In the particular application of diffusion, these random variables are going to actually be our corrupted images so they're going to be noisy images and what the observation means is I could see my corrupted image exactly as it is at 3 at 3 for example. So the observation means I get to actually view that corrupted image at time 3. Whereas if it wasn't observed, then I don't get to know the exact pixels of the corrupted image. I only get to know that I think it should be corrupted by some amount. It should have some noise statistics, but I don't actually get to see the pixels of it. Does that answer your question? Perfect. Other questions? OK, we're gonna get to the derivation then. So there's going to be a lot to cover. We're gonna do it in the following order. We're going to first derive denoising, diffusion probabilistic models. When people think of like classical diffusion, this is what it is, GDPM. This is going to be a bunch of derivations, so we're going to have to first derive how do we add noise to an image to turn it into Gaussian noise and how can I guarantee that it turns into a unique Gaussian noise. Then I'm going to have to make some model assumptions to say how do we start to reverse this and then I'm going to have to derive a loss function because we have to always derive a loss function to optimize and that loss function is going to be an elbow. In fact, the derivation of this elbow is gonna follow the same steps of VAE initially, but then we're going to see that there's going to be a lot more uh math that we have to simplify. All right. After that, uh, derivation, then we're going to talk about practical implementations of units and hyperparametters, then we'll talk about conditional diffusion models, that's how you can, for example, say. Braw me a picture of a white dog on grass and it'll actually generate a picture of a white dog on grass. So the white dog on grass text will be the condition that generates a particular um image. Then we'll talk about some more efficient implementations of diffusion including latent diffusion or it's also called stable diffusion. We're gonna talk about ways that you can do diffusion more quickly that's DDIM. And then we'll talk about some cool applications of diffusion. So this will probably take us to um Monday's lecture on week 5, maybe even into Wednesdays, but that will be how we follow the order of presenting diffusion. All right. We're about to go into the derivation and just like Wasserstein Gans, it's easy to get lost in the weeds. So let me first show you the overall algorithm so you can see how simple it is and then we'll show how we go from all this complex math to this very simple algorithm. So you know what, let me just start to zoom in a bit. That'll help since we don't have the big projector. This is the algorithm for training a diffusion model. This diffusion model is GDPM, so this is um In the denoising. Uh, diffusion Of ballistic models. The training algorithm is as follows. I have some X0 and X0 is my actual image. So this is my image from a training set. So maybe if I'm trying to generate CAR 10 images, this is an image of a dog from a CAR10 training set. Then I'm going to sample a time T, OK? Um, T is gonna be between 1 and big T. The intuition for this that we'll explain more rigorously later is when T is equal to big T, this is like this, uh, when, when little T equals big T, your image is gonna be corrupted to full Gaussian noise. When little T equals 1. I have an image X1, and X1 is Barely corrupted. I've only added a tiny bit of gasoline noise, so the closer little T gets to big T, the more noisy I've fallen in my for diffusion process. Then I'm going to sample some noise Epsilon, which is unitgassian. And then I'm going to take a gradient descent up on the following. So this noise epsilon. It's gonna be this epsilon here. This epsilon is some noise that I've sampled. And what I'm going to do is I'm going to compare the squared error to this term. OK, what is this term? This epsilon data is a neural network. That tries to predict the amount of noise Epsilon. So this is. Let me draw bigger. This is a neural network with parameters data. So this is my Neural network that tries to learn the noise that add to images. This neural network has two inputs. The first is my time step T. And that was what I sampled before because In my neural network. How I subtract noise depends on how far along the diffusion process I am. So if if little T equals big T, right? I am at my pure Gaussian noise and the noise I want to subtract there looks very different from if little T equals 1, where I have a minimally corrupted image and I just want to take out the little bits of little specks of noise that are still on it. And then This is the other input. And This you can think of as the corrupted image. And so X0 is my image. And this epsilon here is the noise that I sampled and so depending on this value alphat with a bar over it, if alphat with a bar over it is close to 1, then this is going to be equal to X0. It's gonna be my perfectly fine image. If alpha T bar equals 0, then this image here is gonna be equal to 0, and then this contribution is gonna be 1 minus 0, so it's gonna be 1 times epsilon. So this is gonna be pure noise. Then alpha T bar being somewhere in between is gonna be those levels of noise uh for which I have a partially noisy image, OK? So our neural network is gonna take our partially noisy image as well as the time along we are the time that that we are along in the diffusion process. And learn to predict the noise that you added, right? If they can learn to predict the noise that you added. And what you can do is If you predicted that noise accurately, you can subtract that noise away from your image to make your image cleaner and cleaner. Question. Uh, the question is, uh, didn't quite catch what Q represents online too. Great. So Q here is a distribution. It's the distribution of natural images. We can't write down what Q is, but Q of X0 will be the distribution, so. In practice, what this means is like what is your data set? So if Q is the distribution of CA 10 images, and we're sampling a CA 10 image from our CAR10 data set. We'll talk about these details further also. You. Yeah, the question is for the alpha T, do we randomly initialize it and is it something that's learned, just something that we update? The answer is no, so alpha T will be a hyperparameter. So we set it at the beginning of the diffusion and uh and we'll discuss exactly how we set that. So I'll go into the details of where all of these come around from. Uh, yes, that's correct. What is, what is your name? Ian's question is, when I say we sample T, does this mean like maybe the first iteration you sample T equals 1, the next iteration you sample T equals 5, maybe the next you sample T equals 500, uh, and this just reflect how noisy the image is. The answer is yes. So basically this epsilon data, this neural network has to learn to denoise the image for any time step. And so this is my way of basically like sampling a random time step to give it an example to learn how to remove noise at that time. Oh, great. The question is, can I redescribe how I said when alpha T. Uh, is a value like 0.5 is a partially noisy image, but when alpha T equals 0, it's a fully noisy image. So the some here reflects my noisy image. I actually didn't finish writing this, so this is my bad. Um, So X0 is my clean image. And Epsilon is noise. So if alpha T bar equals 1, then I'm gonna do X0 plus 0 so that's a fully clean image. If alpha T equals 0.5, right, then I'm gonna have some of my clean image but to it I'm also gonna have added a lot of noise to it and so this is a partially corrupted image and then if alpha T equals 0, then I have no none of my clean image and all noise and so um. What What the actual noisiness of this image will be will depend on the value of alphaar T. Question, other questions? Trust me. Great. The question is, do we decide capital T arbitrarily or is that defined elsewhere? We set the value of capital T. Uh, it is arbitrary but we'll choose the judicious value just as we'll also set the values of alpha T as well. Good question. Other questions? Yes. Uh, the question is, uh, did I say that Epsilon theta is computing the noise that we are subtracting, uh, from our image to, uh, to bring it back to the original image, uh. So Epsilon theta is predict is trying to predict the epsilon that we added to the noisy image. So it's trying to predict basically this term, but just this epsilon here. So it's predicting the noise that we added to our original image. That's correct. Other questions? OK, then let me just say then after training, after we learned the parameters data here, how does diffusion actually work in real time or how does it actually work to generate a new image? So what we do is we start off with unit Gaussian noise. So X at time T. Time T is when we've run our forward process so far that we've turned our image into actual unit Gaussian noise. So what we do is when I want to generate an image, I start off with my unit Gaussian noise. And I do a 4 loop from big T to 1. And at every single iteration, I sample some unique Gaussian noise. And then I compute this equation. All right, we'll derive where this equation comes from, but the key thing to see in this equation is what you're saying is I'm going to take X of T. And try to get to X of T minus 1. Right, X of 0 is my image. X of big T is my unit gassian noise. So X of little T is noisier than X T minus 1. But if I can keep subtracting away noise, so if I keep subtracting away noise, predicting noise from epsilon data, my neural network, and hopefully I have moved XFT closer to X of 0. So I denoise XFT a bit by subtracting off the predicted noise that gives me X of T minus 1. Then I put XFT 1 into my neural network to try to subtract off more noise that gives me XF T minus 2 all the way down to X0. All right, so that's a sampling operation of diffusion question. Yep. OK. Yeah, this is really great. So the question is, in the training algorithm, we can add all the noise directly to X0. So why is it that in the sampling operation, I have to iteratively subtract the noise little by little, I, I have to do a 4 loop that takes big key time steps because if I could subtract all the noise in one time step, then my inference would be a lot faster. I would remove a 4 loop. This is a really great question. So, Uh, the answer to the question will become a lot clearer when we go to the derivations, but let me give you the high-level answer right now, and if you can't follow this, don't worry, we're gonna, it's gonna make a lot more sense when we get to the derivation, so. Our 4 process is not challenging to parameterize, and it is possible to actually write the distribution of any X of T as a function of X of 0 and the noise and so this is actually the correct parameterization of the forward process in the reverse process, um, and there's some intuition about that like we're just adding noise to an image to make it look more like gas gassian noise in the reverse process. The noise that I subtract is going to depend on that very particular image I'm looking at and it's gonna depend on how noisy that image is and so. For that reason, what happens is I have to take that image and then specifically for that particular image, figure out how to subtract off my noise to go back one time set. Now, there are ways to try to combine. Um, ideas of Of subtracting greater noise. And that's going to lead to some of the future algorithms that we talked about, including DDIM where we can basically. Take this 4 loop and only do like every 10 or every 50 iterations, but that will require new ideas. This will be clear when we actually do the math, trust me. Uh, the question is, when you say backwards, do you mean from the noise to the image or from the image to the noise? Yeah, great. Uh, so the question is, is there a correspondence between Uh, an image and then some unique amount of gassian noise. The answer is in the end no, because you'll actually notice the sampling process also has itself a noisy part to it. So even if you start off with the exact same. Gaussian random noise observation on different runs of diffusion, you will get different images. David Yeah. Yeah. Yeah, that's, yeah, David's question is when we return X0, which is which is ideally an image from this distribution, what can we expect? So for this particular algorithm we can only expect the sample from that distribution. So if Q of X0 is my distribution of CA 10 images, I could expect an image that looks like a CA 10 image. And we're going to address this later on because we know for image generation we give a prompt, right? and they give us an image related to that prompt and so that will, that will be where the condition name comes in Edward. Yes. Great, uh, the question from Edward is if the intuition is that we're trying to add, uh we're trying to subtract noise repeatedly from the image to get my clean image, why am I adding some noise back? Hold that question, we may not even actually, we'll probably get to it next lecture. There are two answers here. Uh, one is that it's actually just what is mathematically correct. So when we derive it, we're going to have a variance term that this stigmat Z models. Uh, the other though is that it will give you some stochasticity in the trajectories, um, and that empirically seems to also be a good thing. Question Yeah, the question is, uh, is there a difference, uh, in the quality of the image based off of your initial sample and Uh, the answer should be yes. It should suffer from this similar thing as Gan and VAEs, which is that you rarely sample the outer parts. However, um, Diffusion will be a lot more robust to this because of this iterative subtraction and so even if you have like a very outlier initial noise sample, uh, as you subtract off noise, uh, these XFTs will hopefully be more in distribution to what XFT should look like, right? Let's take a 5 minute break and when we come back we'll start the derivation. Any questions from the break? The one thing I forgot to mention, uh, this alphaar T is a function of T. All right. So it turns out that alphaar T We're gonna define it shortly, but alphaar T. Is a scaler. That starts at 1 When T equals 0, And goes to 0 when she goes to big T. OK. And so Alpha alphaar T is uh decreasing as little T increases and so as little T increases my image will become more and more corrupted. OK. Questions On any of the high-level algorithms. Uh, the question is, does the bar have anything to do with me yet, the bar will be for, uh, for a geometric mean. Other questions. Is the question is, is there a difference between alpha T with and without the bar? Yeah, so, so, uh, I'll just say right now, alphaar T is equal to the product of all the alpha Ts. So, uh, alphaar T will equal uh alpha 1 times alpha 2 times alpha 3 all the way to alpha T. All right. Other questions? OK. Um, And that actually, sorry, as I say that, I shouldn't have a geometric meaning for you. I should say it's the product of, it's the running product of all the uh alpha I. Other questions. OK. We're gonna dive into the details then. Remember, we're gonna do a lot of math, but the the algorithm is simple. It's a squared loss for a neural network that predicts noise. And then when we want to generate an image, we just keep subtracting away noise. All right, um, it is super easy to lose the force for the trees, and so here's our road map. We're gonna start with the forward process. The forward process tells us how to go from an image to unit gassian noise. So we're going to define that reverse, the forward process. We need to define that because later on we're going to reverse it, right? After that, we're going to define our reverse process. And then given this forward and reverse process, we're going to derive a loss function, our elbow, OK? That elbow is going to give us a loss that allows us to optimize the neural network. Then we're going to simplify our elbow because we're going to see it's quite complicated and that elbow simplification is actually going to eventually bring us to this nice easy squared error loss. OK. Um, and then that will be the last function that we can perform stochastic gradient descent on. All right, so we'll start with the forward process. In the forward process, our goal is to transform an image which we're going to call X0. So this is our image. Into Gaussian noise, Z. Z is also going to be written as X at time T. And X of time big T. And Z, they're gonna be equal in distribution. To a unit gas. OK. To do this, we're going to define an iterative process in which we add noise repeatedly to our image and so the graphical model. For the 4 process books as follows. We're gonna have X0. Which is our image. And to that we're going to add noise to get X1. We're gonna add more noise to get X2. And this is gonna continue iteratively until. We arrive at XT and XT should equal a unit gassing and distribution, OK? We know that this distribution is going to factorize as the follows, um, it's gonna be the distribution of X0. I'm gonna call that Q of X0. Times the distribution of X1 given X0, so that's gonna be times Q. Of X1 X0x Q of X2 X1. All the way to Q of XT given X of big T minus 1. All right, so that's gonna be the factorized distribution of this graph. Q X0 is not something that we can optimize for. Actually, none of this will have optimizable parameters, but Q of X0 is the distribution of the images we're trying to generate. But then these Q of XTs given XT minus 1 like QX1 given X0, Q of X2 given X1. What we will be able to do is to find their distribution. And show that for these definitions definitions of these conditional distributions, when I add noise according to these distributions, X of big T will turn into a unit Gaussian, all right. Questions here. OK, so this here is the factorized density of the graph that I just drew. Uh, so this here is the math for what I wrote down here. OK. So we need to define a density of XT given XT minus 1 to run the forward process and really the constraint of this distribution is that it has to be so that when I apply this distribution, when I apply this forward process I get Gaussian noise that's unit Gaussian noise at the end. All right. So We are going to define the four process as follows. um. We're first going to define a hyperparameter beta T. Beta T is going to be between 0 to 1. 4 T equals 1 all the way to big T. Remember, big T is the end of our 4 process where we hope we have Gaussian noise at, OK? We get to choose whatever distribution we want, and there are many ways to take an image and turn it into unit gassian noise. We are going to do it this way, OK? So I'm going to pull this distribution out of the air, but we're going to see why it works. So I'm going to define Q of XT given XT minus 1. As a normal distribution. With this mean And this covariance. All right. OK, again, I admit to you right now it looks like this came out of nowhere. So let's try to unpack this and understand and see if indeed this forward process takes us to Gaussian noise as T goes to infinity, OK. My first question for you. is the following. When I look at this equation, when I look at this distribution. What it's telling me is, given I know XF T minus 1, so XF T minus 1 is gonna be some noisy image that I see. I am building or I am defining a distribution over what XFT is. So XFT is gonna be a noisier image at the next time step, OK? It's gonna be a Gaussian or a normal distribution. We know That's, if I take A noisy signal that's Gaussian. So let's say X of T minus 1 is Gaussian. If I apply just an affine transformation to it. And XFT will also be Gaussian. So this distribution that I'm writing here, it can be written in a simpler way where X of T. It's gonna be some function. Of X of T minus 1. It's actually that there is also going to be an affine function. So I want you to take 30 seconds to a minute to think about how I can write an affine equation. Such that X of T equals some affine transformation of XT minus 1 as well as some noise epsilon that is unit gassian. Such that XFT. Given I have observed X of T minus 1 is gonna have this distribution. All right. So basically what I'm saying is think a bit for 30 seconds, feel free to talk to your neighbor about how to transform this distribution here. Into an actual affine equation and then we'll ask someone to get the answers. All right, I hear some good conversation. I should have prefaced this by saying if you've never done this before, this is a very challenging question, uh, but hopefully when we see the answer you'll see it all makes sense. So does anyone have the equation of how XFT is a function of X of T minus 1 that implements this distribution? Yeah. Perfect, that's the correct answer. OK, so this is the Correct answer for. The affine equation That Reflects this conditional distribution, right? How do we actually show that this is true? After we show this once, you should be able to do this basically by inspection. And so, Remember first that any affine transformation of the Gaussian distribution is Gaussian. We showed that in the VAE lecture, right? When I have a Gaussian distribution, Once I know it's mean and it's covariant, I know the entire distribution. So I never have to do, you know, complicated. Derivations of matching probability distributions. If I want to know what the distribution of XT given XT minus 1 is, all I have to do is calculate what the mean of XT given XT minus 1 is, what the covariance is. Since I know the distribution is Gaussian, the conditional mean and the conditional covariance will describe the entire distribution. Let me write this out and it'll become easier. So let's start off with the conditional means. So this is a distribution. Q of XT given XT minus 1, it's going to be a normal distribution with some mean. The mean I'm gonna call the expected value. Of XT given XT minus 1. And it's gonna have some covariants. The covariance I'm going to write as the covariance of X of T given X of T minus 1. All right. So let's go ahead and just try to compute what expected value of XT given XT minus 1 is for this equation. So if I want to compute the expected value. Of X of T Given X of T minus 1. What I'm going to first do is I'm gonna plug in that X of T equals this equation right here, OK. So this is equal to the expectation. Of square root of 1 minus beta T. Times X of T minus 1. Since the expectation is linear, I'm gonna separate this into two expectations right now, so I'll have my first term. This is conditioned on X of T minus 1. And then I have a plus, the expectation. Of square root of beta T. Times Epsilon. Given that I know X of T minus 1. All right. Let's just consider this term. What is this term equal to? Someone other than you to give me the answer, but what is your name? Neil, someone other than a meal because he knows he gave us the answer. What is this first expectation equal to? Great, it's constant, right? The answer is yes. What is a constant? Yeah, take the E. So what is the expectation say? The expectation says find me the average value of a random variable, but things that are not random come out of the expectation. Beta T is something that I defined beforehand. It's not random. It has some value like 0.1 or 0.2. X of T minus 1 is normally random. What? I have observed it, right? So this is conditioned on knowing what X of T minus 1 is. Once I condition on knowing what X of T minus 1 is, X of T minus 1 is no longer random. So nothing in this expression is random. Therefore, since this is a value, this is some vector, but if X of T minus 1 was a scalar, this would be some value like 5.7. I'm saying what is the expected value of 5.7? It's 5.7. It has no randomness. So the expected value of this expression is equal. To just the value of the expression because it's not random at all. 1 minus beta T times X of T minus 1. What's the expected value of this expression? 0, yeah. How do we know that square root of beta T has some value. It can come outside the expectation. Then we're taking the expected value of some noise that has zero mean, given that I know some image. This image doesn't affect this noise at all. This noise is independent, and so this noise will always have zero mean, and so this has zero mean. So I do a plus 0, and this tells me then if this is my equation relating XF T minus 1 to XFT. Then the conditional mean E of XT given XT minus 1 is square root of 1 minus beta T X T minus 1. That's exactly what this expression is. So I've matched the means, OK. Now I want to match the covariances. So For that, I'm going to compute the covariance. Of XFT, given X of T minus 1. OK. Here, um Since These two are independent. I can write this as the covariances since the noise epsilon is independent of the image X of T minus 1. I can write this as I'll just write this as the covariance of this first term. Uh But this first term We already know if you observe X of T minus 1 has no randomness. Actually, let me, let me write this down. This will be covariance. Of square root of 1 minus beta T. Times XT minus 1, given XT minus 1. Plus covariants of square root of beta T. Epsilon, given X of T minus 1. So just like for this red example, if I know what X of T minus 1 is, X of T minus 1 is deterministic and so is 1 minus beta T. So this is just a number like 5.7. What's the covariance of a number that is not random, it's 0. So this first covariance term equals 0. And for the second term, Again, epsilon and X of T minus 1 are independent, so this is really just the covariance of the square root of beta T times epsilon. Right, we know that when you multiply by a constant, the covariance is going to be that constant squared. Times the covariance of epsilon. The constant squared is equal to beta T. And then the covariance of epsilon equals the identity matrix, right? So the covariance of XFT given XFT minus 1 is beta T times I. And therefore I've matched the covariances. Therefore, Again, because we knew that this was already gassing distribution and it's defined by its mean and its covariance. I've shown that this blue affine equation. I The correct. Affine equation that represents this property distribution, where if I want to know. The distribution of XT Given that I've observed X of T minus 1. This equation here Well actually compute me a sample from that distribution. All right. Questions here. You raise your hand if you're following. OK, great. That's most of the class. All right. So, Given that that is our equation, If we can simplify this equation, Then this will allow us to see what this X of little T look like as big T goes to infinity. Does this actually go to a unique gassian distribution. So what we're going to do is we're going to do some simplification and I have a typo here, there should be a square root. Beta T over here. This is the equation from the prior slide that Emil gave us. All right. Now we're going to define alpha T. So alpha T equals 1 minus beta T. All right. We're gonna do that so that instead of a 1 minus beta T here I have an alpha T. and then we're going to define alphaar T and that's the running product of the alpha Is from I equals 1 to T, OK. Um, in this proof, I'm going to index the epsilons with A subscript key, but The epsilons are IID. They're independent and identically distributed for all T. So every epsilon T is gonna just be a unique Gaussian random variable for all T, and they're going to be independent from each other. We're just gonna do this for the sake of being able to write this out cleanly, OK. So What I want to do in this reparameterization is I'm going to want to write XFT as a function of X0. Right? This is gonna be useful because what this tells us is if I want to know my corrupted image at time XFT. I can just apply one equation. To my initial image x of 0. Rather than having to do a 4 loop, adding noise iteratively from each time, and this is called the reparameterization of the 4 process. This reparameterization allows us to in our algorithm that we saw here. Add noise directly to X of 0 to get X of T. All right, so this is The first part of the answer to Kevin's earlier question, which is, uh, how can we write this as like basically just one step of adding noise to X of 0, we're going to derive that right now. And that means that we don't have to do a for loop for the forward process. So, what I'm going to do, let me zoom in a bit so that it is easier to see. I, I'm going to write. This equation and I'm going to try to write it as a function of X0. How am I going to do that? This function is right now. XFT is now a function of X of T minus 1. I'm going to plug in X of T minus 2 and see what happens. All right. So, Um, X of T equals. So actually, first I'm going to Do my substitution of alpha t equals 1 minus beta T, so that's going to give me X T equals square root of alpha T. Times X of T minus 1. Plus Square root of 1 minus alpha T. Epsilon and I'm gonna put a T minus 1 here. But again, remember all the epsilonte's have a unit gassian distribution. OK. What I'm next going to do is I'm going to plug in. That X of T minus 1. It's going to be square root of 1 minus beta T X T minus 2 plus square, it's going to be equal to square root of 1 minus beta T minus 1 times X of T minus 2 plus square root of beta T minus 1 times X1. So basically I'm just changing all the Ts here to T minus 1. And that gives me this equation. So 1 minus beta T minus 1 is alpha T minus 1, so I'm gonna have a square root of alpha T minus 1. X of T minus 2. Square root of 1 minus alpha T minus 1. Times epsilon of T minus 2. And this is plus square root of 1 minus alpha T. Epsilon T minus 1. OK. Any questions there? Yeah. Great. The question is how did the epsilon from the first line turn to epsilon T minus 1. so I just decided in these lines to assign a subscript to it just so that you could differentiate this epsilon from this epsilon. But these are all the same epsilon or not, they're not the same exact epsilon. They have the same distribution as epsilon. So all these epsilontes are going to be unit gassian, but they're going to be independent. I've only now assigned the subscript so you can see that they have a different epsilons here, that these are different epsilons. These are not the same exact epsilon. At this point I'm also just going to remove this parenthesis, actually I'll write it out. I'm just going to do a distribution, so I'm going to Distribution of the square root alpha T, so square alpha T um Times Alpha T minus 1. X T minus 2. And then I'm going to have plus. Square root of alpha T. Times square root of 1 minus alpha T minus 1, epsilon T minus 2, plus square root of 1 minus alpha T epsilon T minus 1. OK. Any questions here? OK, I'm now going to take these two terms. Which are Failed versions of unit Gaussian noise and combine them and I know that the output is going to be Gaussian because affine transformations of Gaussian random vectors are still Gaussian. So because they are going to be the sum of these two Gaussian random vectors are going to be Gaussian, to know the distribution of the sum of these two, all I have to do is compute their mean and their covariant just like we did before on the prior slide. So I want to combine. These two noise terms. And I know they're gonna be Gaussian distributed. But then their meaning their covariances, I'm gonna have to compute. Right? What is the meaning of the sum of these two? Noise variables, scale noise variables. 0, great, yeah. This has 0 mean and this has 0 means, so the mean is 0. All right. What is the covariance? Uh Right. So, uh, yeah, let's do it turn by turn. So for epsilon T minus 2, its covariance is going to be whatever it multiplies squared. So that's going to be alpha T times 1 minus alpha, T minus 1. Times the covariance of epsilon T minus 2, which is identity. So I have times identity here. And then for this expression, I'm going to have whatever scales squared, so it's going to be 1 minus alphat. Time the covariance of epsilon T minus 1, which is identity. Right. Questions there? Let's just do some algebra then. So I'm going to simplify this. This equals the normal distribution with mean zero. I'll have an alpha T minus alpha T, alpha T minus 1. Plus 1 minus alpha T, all of this multiplying. The identity matrix. And from here we can see that this simplifies the alpha Ts will cancel out and so and so I'm going to get a 1 minus alpha alpha T minus 1. Just to save space, I'm going to just write that here. 1 minus alpha T, alpha T minus 1. Gonna be one Minus alpha T, Alpha T minus 1. And the identity. OK. And what that means is that I can rewrite this equation here. As equal to The square root of um alpha T. Alpha T minus 1 times X of T minus 2, plus The square root Of 1 minus alpha T times alpha T minus 1 times epsilon. Right? Because this term is going to have zero mean and its covariance is going to be this covariance matrix. Questions here. OK, if no questions, then what I want you to see is now I've written XFT in terms of X of T minus 2. And what changes is that instead of just an alpha T here, I have an alpha T times alpha T minus 1, instead of a 1 minus alpha T here, I have a 1 minus alpha T times alpha T minus 1. If you now plug in X of T minus 2 as a function of X of T minus 3 here, you'll further simplify uh all the way down to X0. I'm going to skip all those steps because We already see a pattern The pattern is if I go all the way down to X of 0, I'm going to have alpha T times alpha T minus 1 times alpha T minus 2 all the way down to alpha 1. And so the product of all of the alphas up to alpha T is going to be alphaar T. So this is going to change into alphaar T times X0 plus square root of 1 minus and then this is going to be the same thing. I'm gonna have alpha T, alpha T minus 1 all the way down to alpha 1 multiplying each other. So that's going to give me 1 minus alphaar T. I'ms Epsilon. OK. So this is writing XFT in terms of X0. Questions there. OK, so then here's my question for you. If I were to tell you to write me the distribution, Q of XT given X0, right, we're gonna go in the reverse direction now. We're going to go from an affine equation to a distribution. The first thing that I'm going to note is X of T given X0 is going to be an affine distribution as it's gonna be, it's gonna be a normal distribution because it's an affine transformation of Gaussian random vectors. So it's gonna be normally distributed but with some mean and some covariants. So what is the mean? Given this equation. So, uh, you're correct. This is a zero mean here for this first term, remember X0 is observed. So X0 is no longer zero mean X0 will have some values and so this will be a constant and this will be another constant and so together these two multiplying each other will become the mean. So the mean here is gonna be. The square root Of alpha T X 0. Again, that's because this expression is conditioned on X0, so X0 is no longer random. X0 is gonna be some image. That has some value like 5.7, 6.3, etc. It's not random at all, so this contributes to the name. What is the covariance of the distribution of X of T given X of 0? Perfect. It's 1 minus alphaar T. Is the identity. Right, the reason that we know that is we can take the covariance of this expression. Given X0. Given x0, this term is not random. So the only term that contributes something random is this term. And this covariance is going to be this thing squared times the covariance of epsilon, which is 1 minus alphaar T times identity. All right. And then what you should also see is from this equation here we've derived one element of training. So back in This slide right here, we said that the corrupted image was going to be the square root of alpha T times X0 plus square root of 1 minus alphaar T times epsilon and this equation here is exactly what we've just arrived. Over here. Right, so this takes my perfect image X of 0 and maps it to a corrupt image at time X of T to just a single transformation where if I apply this equation, I get X of T. All right. Questions here. Yeah. Great. The question is, is beta T just an arbitrary sequence of numbers here? That's the perfect transition to the next slide. So, uh, how do you choose beta T? So in the GDPM paper, beta T was chosen in the following way. If I have time. So the X-axis is little T and it goes from 0 to big T and beta T here. It's going from 0 to 1. The prior. But the GDPM paper just defined beta T as follows. The line that goes up. And so, Um, Make big tea over here. OK. And that choice of beta T, that hyperparameter choice of beta T works. There have been later works that have come on, and what they have done is They have used this intuition as follows at time T equals 0, maybe I want to be a bit more careful in adding noise because that's my perfect image, right? But at later time steps, right, when my noise, my image is already mostly noise, I can add more epsilon. I can increase beta even more. So frequently these days when you schedule beta T, it'll actually maybe look a bit like this. One way that you can implement this is via what is called cosine annealing and so when beta T is linear like this, if you compute alphaar T, you get this blue line here, but if you choose, um, if you choose beta T to look more like this, then you get this cosine annealing and so alphaar T will start off higher and then and then go down, uh, more gracefully again this reflects that maybe you want to not add that much noise at the early time steps. And then in the later time steps when your image is already almost towards gasing noise, you can up the noise that you add. You're adding more noise to something that already looks noisy, OK? And this orange line tends to have better empirical performance. Today there are even more fancy ways to uh do your scheduling including different re reparameterizations into like velocity vectors and whatnot. We won't cover those in this class, but uh in this class uh we'll cover this linear and this cosine and kneeling. Questions here. OK. The other thing I want you to then see. Is that As Big T goes to infinity. What does this distribution turn into? What you should see is as big T goes to infinity, right, Alphaar T. Eventually goes to zero and so it's going to be 0 times X0. That's gonna be 0 mean and it's gonna be 1 minus 0 times identity and that's just equal to identity. So as big T goes to infinity, we in fact do see that we've done what we want to in the forward process, which is as T goes to infinity, I turn X of big T into a unit gassian distribution. Right? And so this is also the proof that when I chose this equation for how I'm going to iteratively add noise to every image. I was able to turn that image into pure Gaussian noise, and then we also have this added benefit. I don't have to do this thing iteratively. You could just tell me what time step T I want you to generate a noisy image for. Give me my clean image, and if I just apply this equation, that will give me my noisy image at time T. Right, that's a lot of questions here. Yeah, Oh yeah, great question is, uh, I skipped a few steps in going from X T minus 2 to X0. How did I do that? So, um, you could, so If you recursively follow this down all the way down to X0, then you'll get this equation. What I didn't do is I didn't show like you could do a square root of alpha T. Alpha T minus 1 and then for X of T minus 2, plug in square root of alpha T minus 2 times X of T minus 3 plus square root of 1 minus alpha T minus 2. Uh, epsilon T minus 3 plus. And so if you keep following this recursively down, then you eventually get this equation over here. Thanks for that clarifying question. Other questions. All right. And that is the forward process and that tells us then how we add. Noise to an image. To progressively turn it into a unit gassian distribution. All right. That's actually the quote unquote easy part of diffusion. Let me make sure the projector comes back on. Are there any questions on this board process? OK, let's get to the harder thing then, which is, once we've defined this forward process. How can I actually go ahead? And reverse it. So the first thing that we need to do is we need to define a reverse process, right? So we've done #1 the forward process. Now we're doing number 2, define the reverse process. This is actually just going to be a single slide because this is going to be an assumption that we make on our model. And After that, given the assumption that we made, given the distribution that we choose, we're going to divide the elbow. OK, so first, the reverse process, how are we going to define it? We're gonna define it as follows. I'm gonna draw the graph of the reverse process. What happens in the reverse process is that we start off. From our unit gassian noise. So X of big T is my unique gassian noise. Into it I am going to start to. Subtract away noise. According to some distribution. Some uh equation. That eventually is gonna take me. Back to my perfect image, or sorry, an image from my distribution X0. OK. We know that this graph factorizes as follows. It's going to be PF X for big T. Times of X, so big T minus 1 given X of big T. Times P of X. The big T minus 2 given X the big T minus 1. And what that means is that if I want to parameterize. This reverse diffusion process. I need to be able to write down what these distributions are. All right. So PFXT we already know PFX big T, that's my unit gassian distribution. But I need to know, or I need to assume a form of PF X T minus 1 given X of big T or PF X of big T minus 2 given X of big T. In general, I need to parameterize this this distribution P of little XT minus X T minus 1 given X of little T, OK. I'm just going to tell you the answer for what we select, and I'll give you three reasons why it's reasonable. So we need to be able to write this as some density, and I'm just going to choose it to be a unit, a Gaussian distribution where the mean and the covariance are parameterized by neural networks. So this should look very similar to the VAE, right? So what I'm saying is if I want to go from a noisier image XFT to a less noisy image X of T minus 1. I'm going to pass my image XFT into two into two neural networks. Those two neural networks will return to me the distribution mean and the distribution covariance of X of T minus 1. And then from there I could sample an X of T minus 1. All right. Why did I choose to be a normal distribution? So first, Normal distribution is easy to work with. We've seen that for all of our generative models. People like to choose normal distribution. So that's reason number one. Reason number 2 is In my forward process, my Q of XTs given XT minus 1s were also Gaussian. We're gonna have expressions that have both the Q's and the P's and the cues are already Gaussian, so if I choose the pieces to be Gaussian, I'll match the distributions and we'll have some, uh some niceDs from there. OK, the third reason is the more fundamental reason that I'm going to say it's beyond the scope of this course, but if you're interested in it, uh, you should take a stochastic processes course. So, Um, if you try to reverse a stochastic process, like, for example, Browning in motion. It turns out that if you're trying to reverse this brownie in motion, there are uh As you make the time step smaller between reverse steps and going from the state of molecules at some time T to just a tiny bit before time T minus 1, the Gaussian distribution is actually a reasonable assumption of how to reverse Brownian motion in a very small time window, OK. Don't worry if you didn't follow that. That's beyond the scope of this course. This is a justification to say that this is not an unreasonable assumption, but, uh, even if that Brownian motion thing wasn't true, we would probably still just choose a normal distribution because uh then we're just going to put it on the neural networks. You and Sigma to learn something. Of high capacity enough to model the distribution of X of T minus 1 given X of T. All right. Questions there. OK, so that is step 2, reverse process. I needed to parameterize the reverse process and define it. The definition of the reverse process was this graphical model where at every single step we're going to subtract away some of the noise. And to write out this reverse process, I needed to write. This, the reverse process graph factor rises according to this distribution and then to be able to actually write this out, I have to define PXT minus 1 given X of T. This is a free choice to me and I just chose it to be a Gaussian distribution, a normal distribution because Those are easy distributions to work with, right? And they are not reasonable assumptions and empirically they're going to work out as well. All right, so that's step 2. Now we come to probably the most involved part, which is the elbow derivation. All right. So we have to find a 44 process. Those are my cues. We defined the graph of the reverse process. And we define that in this reverse process. These distributions I need to know to write out this density are going to be Gaussian with neural network means and neural network covariances just like in the VAE. Now, I need a way to set the parameters of data so I can actually reverse this distribution well, reverse this, uh sorry, reverse this diffusion process well, reverse the forward process well. And so to do that, I need a loss function. That tells me how do I change data. So that These neural network means and covariances are actually doing the work of changing pure Gaussian noise into an actual image for my distribution, right? So I need a loss function to optimize these data. Questions there. All right. This will be that loss function derivation. So today, we're just going to do a few steps of it. We're going to finish this next time, but we'll at least do the first few steps. When I taught this last year, um, People got lost really quickly, so I totally revamped these slides and basically for every single step I'm going to tell you the why of where this step comes from. So literally for every single step, it'll be followed by a Y and I'll write out the the the the steps. So you guys will truly understand or be at least have the notes to understand every single step of this derivation, right? And so, um, just like the VAE, where do we start from? We start from log likelihood. Since we're calling this a loss, I'm going to put a minus sign and say it's gonna be minus, uh, log likelihood, negative log likelihood, all right. Um, this is, this expression right here is a negative log likelihood of an image X0 under our model distribution under our entire reverse diffusion process with neural network parameters data that denoise images. OK, so we would, we would like to make this X0, this image that we generate. As likely, sorry, this X this is not an image that we generate X0 is an image from like CA 10 for example. We would like to change our model parameters data so that when I put an actual image from CA 10, it has high likelihood under my model parameters. OK, so this is where we're going to start off. This is always the or this is the lost function of maximum likelihood that we used last quarter for. Our softmax classifiers and we used uh this quarter for VAEs right. This Expression is intractable to right out. Because just like the VAE's, if you try to actually write out the product of all of these Gaussians that are parameterized by neural networks. You won't have an expression that you can actually just write down by hand and so because we cannot evaluate this expression tractably we have to do exactly what we did for VAE, which is to write a tractable lower bound to it and then we're going to try to optimize that lower bound. Questions there. OK. The first few steps of this are going to look exactly like the VAE derivation. So the first thing that I'm going to do is I'm going to wrap this negative log likelihood and an expectation under this distribution Q of X1 to T given X0. Right, this Q of X1 to T given X0, that's actually the same Q that parameter, that's the same that describes the forward process of diffusion. The way we can do this is exactly the way we did it for the VAE. This is the expected log likelihood trick. So P theta of X0 does not depend on. X1 X2, X3, all the way to big XT. And so it can come outside of the expectation. In other words, if I take this expression and I read it out. This is going to be written over here. Um, so this expectation is going to be the integral of Q of X1 to T given X0 since that's the distribution I'm taking the expectation with respect to. I'm going to have a minus log p theta. And then I have to integrate over all possible values of the distribution. So my integral will be over all values X1 to XT. This log p theta X0 does not have any X1 to XT, and so it can come outside of the integral. So I'm going to bring minus log p theta X0 outside of the integral, and now I'm integrating a probability distribution over random vectors X1 to XT. Integrating out all X1 to XT. So by law of total probability, this thing equals 1 and therefore this expected log likelihood equals the actual negative log likelihood. Questions there. OK, that's step one, we'll do one more step here. So, um, the next step will be Again, the same stuff that we did in the VAE, we're now going to write out P theta X0 via my chain rule. And so this comes from the chain rule of probability. I know that if I write um P X 0 to T. So why am I writing X0 to T? I'm wanting to incorporate all the images across all time into this expression to make simplifications later. So PX0 to T is by the chain rule p theta of X0 times p theta of the remaining variables X1 to T given X of 0. So then I divide both sides by this expression and then I give you p of X0 equals PX0 to T divided by P X quantity given X0. Questions there. All right, so we'll come back on Wednesday and we'll continue on this proof.
